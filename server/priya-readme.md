Notes
Within the ignore directory, I placed older version of code I was working with and no longer need.

Prerequisites
- postgres. add psql to PATH if want to query db directly via windows cmd.
- for clustering: pip install scikit-learn nltk matplotlib cleanco levenshtein name_matching 
- for db setup: pip install orjson psycopg2

Mini DB Setup
I pulled data from one json file for recall and event data. These files would be under sample-data directory, but are too large for github.
- init_db.py: sets up local postgres db with table params (feel free to change)
--> this just needs to be run once. Source: db/dbinit.py

- dataloader.py: reads files in sample-data path and inserts into tables generated by init_db.py
--> this is modified version of db/dataloader.py
--> run this one time locally to ensure postgres db is set up

- get_data.py: creates DFs from recall, device_event, and device tables from db once dataloader.py has run
--> this code is supposed to be leveraged when clustering

Cluster Code
The notebook cluster_similarity.ipynb explores pulling the data from the db from get_data.py and performing preprocessing and clustering. There are multiple ways that cluster was performed, but due to time constraints, I found using the pacakge name_matching was useful. It takes about 15 mins to compare recall and event manufacturers to generate matches, but it just needs to be run once. I also saved the resulting "joined" df into a pickle file so we can open it without have to rerun it (combined_df.pkl).

I trained a kmeans cluster on the combined set of recall and event data. Training is quick and the model is saved to a pickle file too (kmeans.pkl). However, clustering is known to be time expensive when testing so I wasn't able to generate results from the model quickly enough. Also, I arbitrarily chose 1000 clusters. 

NOTE: the pkl files were too large and had to be removed from the push.

Streamlit
- Home.py, pages/adverse events.py, pages/recalls.py
- to run: python -m streamlit Home.py
--> dashboard will open on port 8051

I need to portray the combined cluster data somehow in the Home page. 
